# -*- coding: utf-8 -*-
"""mbcproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6Qg7VS16vklZ1uWL1E5Zw9bPfsFTmCP
"""

!pip install keras==2.2.5
!pip install onnxruntime
!pip install keras2onnx

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from os import listdir

from keras.preprocessing import sequence
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

from keras.optimizers import Adam
from keras.models import load_model
from keras.callbacks import ModelCheckpoint

import keras
import keras2onnx
import onnx

import onnxruntime

def getData(dirpath):
  result = []

  for csvfile in listdir(dirpath):
    curr_csv = np.genfromtxt(dirpath + csvfile,delimiter=",")
    curr_csv = np.delete(curr_csv, 0, 0)
    curr_csv = np.delete(curr_csv, 0, 0)
    if curr_csv.shape[1] <= 20:
      to_append = np.transpose(np.array([np.array(curr_csv[:,-1]).T]))
      while curr_csv.shape[1] < 20:
        curr_csv = np.hstack((curr_csv,to_append))
      result.append(np.transpose(curr_csv))
  return result

def common_member(a, b): 
    a_set = set(a) 
    b_set = set(b) 
    if (a_set & b_set): 
        print(a_set & b_set) 
    else: 
        print("No common elements")

allG1 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G1/")
allG1.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G1/"))
allG1.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G1/"))
allG1.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G1/"))

allG2 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G2/")
allG2.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G2/"))
allG2.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G2/"))
allG2.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G2/"))

allG3 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G3/")
allG3.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G3/"))
allG3.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G3/"))
allG3.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G3/"))

allG4 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G4/")
allG4.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G4/"))
allG4.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G4/"))
allG4.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G4/"))

allG5 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G5/")
allG5.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G5/"))
allG5.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G5/"))
allG5.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G5/"))

allG6 = getData("/content/drive/My Drive/mbfinaldata/newdata/data1/G6/")
allG6.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data2/G6/"))
allG6.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data3/G6/"))
allG6.extend(getData("/content/drive/My Drive/mbfinaldata/newdata/data4/G6/"))

G1_target = np.full(shape=len(allG1), fill_value=0, dtype=np.int)
G2_target = np.full(shape=len(allG2), fill_value=1, dtype=np.int)
#G3_target = np.full(shape=len(allG3), fill_value=2, dtype=np.int)
#G4_target = np.full(shape=len(allG4), fill_value=2, dtype=np.int)
#G5_target = np.full(shape=len(allG5), fill_value=2, dtype=np.int)
G6_target = np.full(shape=len(allG6), fill_value=3, dtype=np.int)

allX = allG1
allX.extend(allG2)
#allX.extend(allG3)
#allX.extend(allG4)
#allX.extend(allG5)
allX.extend(allG6)
#allX = np.array(allX)

allY = np.concatenate((G1_target, G2_target, G6_target), axis=0)

allY = np.concatenate((G1_target, G2_target, G3_target, G4_target, G5_target, G6_target), axis=0)

allX = np.array(allX)

print(allX.shape)
print(allY.shape)

randomize = np.arange(243)
np.random.shuffle(randomize)
allX = allX[randomize]
allY = allY[randomize]
print(allX.shape)
print(allY.shape)

trainingX = allX[:195]
trainingY = allY[:195]
testingX = allX[195:]
testingY = allY[195:]
print(trainingX.shape)
print(trainingY.shape)
print(testingX.shape)
print(testingY.shape)

model = Sequential()
model.add(LSTM(256, input_shape=(20, 6)))
model.add(Dense(4, activation='sigmoid'))

from keras.utils import to_categorical
y_binary = to_categorical(trainingY)
testingY_binary = to_categorical(testingY)

print(allY)

adam = Adam(lr=0.009)
chk = ModelCheckpoint('best_model.pkl', monitor='val_acc', save_best_only=True, mode='max', verbose=1)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
model.fit(trainingX, y_binary, epochs=60, batch_size=128, callbacks=[chk], validation_data=(testingX,testingY_binary))

onnx_model = keras2onnx.convert_keras(model, model.name)
onnx.save(onnx_model, 'model3.onnx')

sess = onnxruntime.InferenceSession('model3.onnx')
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name

result = sess.run([output_name], {input_name: [})

for idx,x in enumerate(allX):
  input_name = sess.get_inputs()[0].name
  output_name = sess.get_outputs()[0].name
  result = sess.run([output_name], {input_name: [x]})
  result_list_orig = np.array(result).tolist()
  result_list = result_list_orig[0][0]
  pred = result_list.index(max(result_list))
  if result_list[2] > 0.97 and result_list[3] > 0.97:
    pred = 3
  if pred != allY[idx]:
    print("miss")
    print(result_list_orig)
    print(pred)
    print(allY[idx])

import json
with open("/content/test.json") as f:
  data = json.load(f)
data = json.dumps(data)
print(data)

import numpy as np
def preprocess(input_data_json):
    # convert the JSON data into the tensor input
    accel_dict = np.array(json.loads(input_data_json)['accel'])
    gyro_dict = np.array(json.loads(input_data_json)['gyro'])

    acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z = [], [], [], [], [], []
    for time_stamp_idx in range(min(len(accel_dict), len(gyro_dict))):
        if time_stamp_idx < 20:
                curr_acc_time_stamp = accel_dict[time_stamp_idx]
                curr_gyro_time_stamp = gyro_dict[time_stamp_idx]
                acc_x.append(curr_acc_time_stamp.get('x'))
                acc_y.append(curr_acc_time_stamp.get('y'))
                acc_z.append(curr_acc_time_stamp.get('z'))
                gyro_x.append(curr_gyro_time_stamp.get('x'))
                gyro_y.append(curr_gyro_time_stamp.get('y'))
                gyro_z.append(curr_gyro_time_stamp.get('z'))

    while len(acc_x) < 19:
        acc_x.extend(acc_x[-1])
        acc_y.extend(acc_y[-1])
        acc_z.extend(acc_z[-1])
        gyro_x.extend(gyro_x[-1])
        gyro_y.extend(gyro_y[-1])
        gyro_z.extend(gyro_z[-1])

    trial_info = np.array([np.array(acc_x), np.array(acc_y), np.array(acc_z), np.array(gyro_x), np.array(gyro_y), np.array(gyro_z)]).astype('float32')
    trial_info = np.transpose(trial_info)

    return np.array([trial_info])

out = preprocess(data)
out.shape

r = sess.run([output_name], {input_name: out})
print(r)

r[0][0]

int(np.argmax(r[0][0], axis=0))

import onnx
def printModel(model):
    
    for input in model.graph.input:
        print (input.name, end=": ")
    # get type of input tensor
        tensor_type = input.type.tensor_type
    # check if it has a shape:
        if (tensor_type.HasField("shape")):
        # iterate through dimensions of the shape:
            for d in tensor_type.shape.dim:
            # the dimension may have a definite (integer) value or a symbolic identifier or neither:
                if (d.HasField("dim_value")):

                    print (d.dim_value, end=", ")  # known dimension

                elif (d.HasField("dim_param")):
                    print (d.dim_param, end=", ")  # unknown dimension with symbolic name
                else:
                    print ("?", end=", ")  # unknown dimension with no name
        else:
            print ("unknown rank", end="")
        print()
model = onnx.load("/content/model2.onnx")
printModel(model)

def preprocess(input_data_json):
    # convert the JSON data into the tensor input
    accel_dict = np.array(json.loads(input_data_json)['accel'])
    gyro_dict = np.array(json.loads(input_data_json)['gyro'])
    
    acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z = [], [], [], [], [], []
    for time_stamp_idx in range(min(len(accel_dict), len(gyro_dict))):
        if time_stamp_idx < 20: 
                curr_acc_time_stamp = accel_dict[time_stamp_idx]
                curr_gyro_time_stamp = gyro_dict[time_stamp_idx]
                acc_x.append(curr_acc_time_stamp.get('x'))
                acc_y.append(curr_acc_time_stamp.get('y'))
                acc_z.append(curr_acc_time_stamp.get('z'))
                gyro_x.append(curr_gyro_time_stamp.get('x'))
                gyro_y.append(curr_gyro_time_stamp.get('y'))
                gyro_z.append(curr_gyro_time_stamp.get('z'))

    while len(acc_x) < 19: 
        acc_x.extend(acc_x[-1])
        acc_y.extend(acc_y[-1])
        acc_z.extend(acc_z[-1])
        gyro_x.extend(gyro_x[-1])
        gyro_y.extend(gyro_y[-1])
        gyro_z.extend(gyro_z[-1])

    trial_info = np.array([np.array(acc_x), np.array(acc_y), np.array(acc_z), np.array(gyro_x), np.array(gyro_y), np.array(gyro_z)]).astype('float32')
    trial_info = np.transpose(trial_info)
    trial_info = np.array([trial_info])

    return trial_info